{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Identify the data cleaning phase and each stage from an expert’s perspective.\n",
    "- Implement solutions for common data structure issues programmatically\n",
    "- Implement cleaning for dirty and messy data with Python for tabular data, time series, and text data.\n",
    "- Implement visual and programmatic testing\n",
    "- Store cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual vs. programmatic cleaning\n",
    "\n",
    "Programmatic data cleaning is faster, more efficient, and a better fit for reproducible data wrangling workflows than manual data cleaning. Data wrangling takes tremendous time for the data professional, so programmatic data cleaning is strongly preferred compared to the manual process.\n",
    "The Data Cleaning Process\n",
    "\n",
    "Programmatic data cleaning has three steps:\n",
    "\n",
    "- Define: write a data cleaning plan by converting your assessments into cleaning tasks.\n",
    "- Code: translate the data cleaning plan to code and run it to clean the data.\n",
    "- Test: test the dataset, often using code, to ensure the cleaning code works and revisit some of the elements from the assessment phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring issues and fixing them\n",
    "\n",
    "Let's recap the data structuring issues and remediation techniques:\n",
    "Issue: Column headers are values, not variable names\n",
    "- Use **unpivoting** or melting to convert a wide dataset (with many columns) into a long format (with fewer columns and more rows).\n",
    "- Use **transposing** to switch rows with columns and columns with rows.\n",
    "\n",
    "Issue: Get a clearer understanding of the groupings in data.\n",
    "- Use **pivoting** to convert a long dataset into a wide dataset and create useful groupings.\n",
    "- Use **group-by and aggregations** to group categorical data together and aggregate the values associated with them like a sum or a mean.\n",
    "\n",
    "Issue: A single observational unit is stored in multiple tables\n",
    "- Use **merging** to combine multiple data tables into a single table with all of the required information\n",
    "- Use **appending or concatenating** on two tables with the same variables and add the data points from one table to another table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Unpivoting:** Using pandas' `.melt()`, specifying id_vars as the identifier variable, var_name as the name of the new column with the original table's variable name, and value_vars as the name of the new column with the original table's values.\n",
    "\n",
    "- **Pivoting:** Using `.pivot()` method with index, columns, and values arguments to produce groupings.\n",
    "\n",
    "- **Transposing:** Using the `.T` method.\n",
    "\n",
    "- **Merging:** Using the pandas `.merge()` method. By default, `.merge()` performs an inner join, using the intersection of keys from both frames and preserving the order of the left keys.\n",
    "\n",
    "- **Appending:** Using `.concat()` method. We can use this method with the parameterignore_index=Trueto ensure the index is labeled 0 to n-1, where n is the total number of rows.\n",
    "\n",
    "- **Group-by and aggregation:** Using `groupby()` and `.agg()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Mergeing \n",
    "Advanced Merging Example:\n",
    "\n",
    "To perform more advanced pandas merging operations, you can use the onand howarguments to the merge() function.\n",
    "\n",
    "1. `On` specifies the column-level names to join on. You can specify multiple columns as long as these are found in both Dataframes.\n",
    "2. `how` specifies the type of merge to be performed.\n",
    "3. Furthermore, `left_on` and `right_on` specifies which dataframe's index (left or right) to use as the join key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers\n",
    "\n",
    "- Set up a range manually if you already know the range pandas indexing\n",
    "- Identify outliers automatically using the standard deviation method, using the outputs of pandas df.describe() function.\n",
    "- Drop outliers using df.drop(index=...)\n",
    "    - Recall you can use the inplace=argument to simplify your implementation by dropping the rows in place, which overwrites your original data.\n",
    "- Finally, identify the impact on summary statistics after dealing with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "- `.dupliated() .drop_duplicates() .drop()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "It's important to identify if the missing values in the dataset are correctly represented. Sometimes the missing values are represented as characters like \"-\" and \"#\" or texts like \"no data\", which can be easily missed using the .isna() method. So we should always check if missing values are correctly represented and replace them with proper values like np.nan. Some useful methods are:\n",
    "- `.isin()`\n",
    "- `.replace()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly recap the options to deal with these NaNs:\n",
    "\n",
    "1. Drop the rows of your dataset if there are only a few rows with missing values. You can use pandas' `dropna()`, and it won’t impact your data analyses significantly.\n",
    "2. Drop an entire column when almost all of the values (such as 95-98% of values) are missing in a column. You can use `df.drop('COLUMN_NAME', axis=1)`.\n",
    "3. If you don’t want to drop the existing data, impute these values using pandas' `df.fillna()` function. For example, to impute the values using the mean of the column, use `df['COLUMN_NAME'].fillna( df['COLUMN_NAME'].mean())`\n",
    "4. Convert the data into categories using `pd.cut()`, then apply one of the above techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking data quality, it is usually best to deal with completeness issues first so that subsequent data cleaning around missing data will not have to be repeated.\n",
    "\n",
    "We looked at ways to remediate the following tidiness issues in the data tables.\n",
    "\n",
    "- **Multiple variables** are stored in one column. String operations and unpivoting can help us resolve this type of issue.\n",
    "- **One observation** unit is stored in multiple tables. We can use merging to solve the issue.\n",
    "\n",
    "We looked at how to remediate some major data quality issues in our clinical trial data tables, including using operations like str.strip(), astype(), pandas indexing, and more!\n",
    "\n",
    "This concludes our exploration of data quality issues in this demo. Note that we didn’t get to all the data quality issues - some issues we didn't cover include:\n",
    "\n",
    "- **Validity issues** where sometimes state names are fully mentioned whereas other times abbreviations are used.\n",
    "- Multiple phone number formats."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
