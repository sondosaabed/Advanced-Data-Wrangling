{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "- Identify the data cleaning phase and each stage from an expert’s perspective.\n",
    "- Implement solutions for common data structure issues programmatically\n",
    "- Implement cleaning for dirty and messy data with Python for tabular data, time series, and text data.\n",
    "- Implement visual and programmatic testing\n",
    "- Store cleaned data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual vs. programmatic cleaning\n",
    "\n",
    "Programmatic data cleaning is faster, more efficient, and a better fit for reproducible data wrangling workflows than manual data cleaning. Data wrangling takes tremendous time for the data professional, so programmatic data cleaning is strongly preferred compared to the manual process.\n",
    "The Data Cleaning Process\n",
    "\n",
    "Programmatic data cleaning has three steps:\n",
    "\n",
    "- Define: write a data cleaning plan by converting your assessments into cleaning tasks.\n",
    "- Code: translate the data cleaning plan to code and run it to clean the data.\n",
    "- Test: test the dataset, often using code, to ensure the cleaning code works and revisit some of the elements from the assessment phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring issues and fixing them\n",
    "\n",
    "Let's recap the data structuring issues and remediation techniques:\n",
    "Issue: Column headers are values, not variable names\n",
    "- Use **unpivoting** or melting to convert a wide dataset (with many columns) into a long format (with fewer columns and more rows).\n",
    "- Use **transposing** to switch rows with columns and columns with rows.\n",
    "\n",
    "Issue: Get a clearer understanding of the groupings in data.\n",
    "- Use **pivoting** to convert a long dataset into a wide dataset and create useful groupings.\n",
    "- Use **group-by and aggregations** to group categorical data together and aggregate the values associated with them like a sum or a mean.\n",
    "\n",
    "Issue: A single observational unit is stored in multiple tables\n",
    "- Use **merging** to combine multiple data tables into a single table with all of the required information\n",
    "- Use **appending or concatenating** on two tables with the same variables and add the data points from one table to another table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Unpivoting:** Using pandas' `.melt()`, specifying id_vars as the identifier variable, var_name as the name of the new column with the original table's variable name, and value_vars as the name of the new column with the original table's values.\n",
    "\n",
    "- **Pivoting:** Using `.pivot()` method with index, columns, and values arguments to produce groupings.\n",
    "\n",
    "- **Transposing:** Using the `.T` method.\n",
    "\n",
    "- **Merging:** Using the pandas `.merge()` method. By default, `.merge()` performs an inner join, using the intersection of keys from both frames and preserving the order of the left keys.\n",
    "\n",
    "- **Appending:** Using `.concat()` method. We can use this method with the parameterignore_index=Trueto ensure the index is labeled 0 to n-1, where n is the total number of rows.\n",
    "\n",
    "- **Group-by and aggregation:** Using `groupby()` and `.agg()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Mergeing \n",
    "Advanced Merging Example:\n",
    "\n",
    "To perform more advanced pandas merging operations, you can use the onand howarguments to the merge() function.\n",
    "\n",
    "1. `On` specifies the column-level names to join on. You can specify multiple columns as long as these are found in both Dataframes.\n",
    "2. `how` specifies the type of merge to be performed.\n",
    "3. Furthermore, `left_on` and `right_on` specifies which dataframe's index (left or right) to use as the join key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle Outliers\n",
    "\n",
    "- Set up a range manually if you already know the range pandas indexing\n",
    "- Identify outliers automatically using the standard deviation method, using the outputs of pandas df.describe() function.\n",
    "- Drop outliers using df.drop(index=...)\n",
    "    - Recall you can use the inplace=argument to simplify your implementation by dropping the rows in place, which overwrites your original data.\n",
    "- Finally, identify the impact on summary statistics after dealing with outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicates\n",
    "- `.dupliated() .drop_duplicates() .drop()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values\n",
    "It's important to identify if the missing values in the dataset are correctly represented. Sometimes the missing values are represented as characters like \"-\" and \"#\" or texts like \"no data\", which can be easily missed using the .isna() method. So we should always check if missing values are correctly represented and replace them with proper values like np.nan. Some useful methods are:\n",
    "- `.isin()`\n",
    "- `.replace()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly recap the options to deal with these NaNs:\n",
    "\n",
    "1. Drop the rows of your dataset if there are only a few rows with missing values. You can use pandas' `dropna()`, and it won’t impact your data analyses significantly.\n",
    "2. Drop an entire column when almost all of the values (such as 95-98% of values) are missing in a column. You can use `df.drop('COLUMN_NAME', axis=1)`.\n",
    "3. If you don’t want to drop the existing data, impute these values using pandas' `df.fillna()` function. For example, to impute the values using the mean of the column, use `df['COLUMN_NAME'].fillna( df['COLUMN_NAME'].mean())`\n",
    "4. Convert the data into categories using `pd.cut()`, then apply one of the above techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When checking data quality, it is usually best to deal with completeness issues first so that subsequent data cleaning around missing data will not have to be repeated.\n",
    "\n",
    "We looked at ways to remediate the following tidiness issues in the data tables.\n",
    "\n",
    "- **Multiple variables** are stored in one column. String operations and unpivoting can help us resolve this type of issue.\n",
    "- **One observation** unit is stored in multiple tables. We can use merging to solve the issue.\n",
    "\n",
    "We looked at how to remediate some major data quality issues in our clinical trial data tables, including using operations like str.strip(), astype(), pandas indexing, and more!\n",
    "\n",
    "This concludes our exploration of data quality issues in this demo. Note that we didn’t get to all the data quality issues - some issues we didn't cover include:\n",
    "\n",
    "- **Validity issues** where sometimes state names are fully mentioned whereas other times abbreviations are used.\n",
    "- Multiple phone number formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing Data with Scikit-Learn pipeline\n",
    "\n",
    "**Scaling data**\n",
    "\n",
    "In many real-world cases, the scale or range of units applying for one variable in our dataset doesn’t apply to others. Standardizing data enables our data to follow a standard scale. We can use scalers from sklearn, such as sklearn’s `StandardScaler(), MinMaxScaler(), or RobustScaler()` - which is robust to outliers, to help us with this.\n",
    "\n",
    "**Encoding data**\n",
    "\n",
    "Encoding is a popular technique used in many cases, including transforming categorical data into numerical data. We looked into the `Ordinal Encoder and One-hot encoding` as solutions for this.\n",
    "\n",
    "**Imputing missing data**\n",
    "\n",
    "`SimpleImputer()` is one way to impute numerical and categorical variables. There are other options available for imputing data in sklearn, such as transforming a dataset into a binary matrix indicating the presence of missing values in the dataset. You can find more information about these options in the sklearn documentation: Imputation of missing values, including how to impute categorical variables.\n",
    "\n",
    "**Pipelines**\n",
    "\n",
    "Finally, we looked at sklearn `Pipelines` as a handy way of assembling a bunch of sklearn operations in a determined flow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text data\n",
    "\n",
    "- **Normalization + additional cleaning.**\n",
    "\n",
    "Normalization involves getting text into a standard format (e.g., converting sentences into lowercase).\n",
    "\n",
    "- Additional cleaning operations can include:\n",
    "    - Removing words that aren’t relevant to us, like “a”, “the”, and “is” that are called stopwords.\n",
    "    -  Reducing words to their root base, which means turning all mentions of going and gone to go). It is also known as stemming.\n",
    "\n",
    "- **Tokenization**\n",
    "\n",
    "In this stage, you divide your text into individual tokens from your dataset.\n",
    "Vectorization\n",
    "\n",
    "In this stage, you convert these tokens into a machine-readable format so they can then for example be used to train machine-learning models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Series data\n",
    "\n",
    "several operations you can perform to handle time series data in Python.\n",
    "\n",
    "Some key considerations to remember are:\n",
    "\n",
    "- Convert dates as pandas datetime objects using `pd.to_datetime(COLUMN_NAME)`\n",
    "- Set your dataframe's index as the date/time column to turn the index into a Datetime index for more advanced functionality using `df = df.set_index(COLUMN_NAME)`\n",
    "- Use resampling with time series to provide a time-based grouping with a target frequency, and aggregate based on that.\n",
    "    - For example, to get the yearly (Y) mean of values in the \"COLUMN_NAME\" column, use: `df.resample(\"Y\")[COLUMN_NAME].mean()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "Testing is one of our final stages in data cleaning, and it can involve visually and programmatically assessing our dataset.\n",
    "\n",
    "- **Visual methods**\n",
    "    - Heatmaps to visualize missing data\n",
    "    - Histograms to visualize the range of data\n",
    "    - Box plots to visualize outliers\n",
    "\n",
    "- **Programmatic methods**\n",
    "    - Use Python assert statements to check the data types, number of NA values, etc."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
